{
  "hash": "f87d40b113dd5a7b1fb8d0eb86617540",
  "result": {
    "markdown": "---\ntitle: \"Générer des graphes de mot avec R.temis\"\n\ncategories:\n- Analyse textuelle\n\nauthor: \n  - name: \"Coralie Cottet\"\n    affiliations:\n      - name: \"Ensai-Ined\"\n\ndate: 07/25/2023\n\nimage: \"http://rtemis.hypotheses.org/files/2019/01/2019.01.21.BGarnier_Pas_%C3%A0_Pas_sous_R.temisStudio_V0_html_4395b9b1d02c56de.png\"\n\nformat: \n  html: default\n\nfilters:\n  - lightbox\nlightbox: auto\n\nabstract: |\n Pour réaliser un nuage ou un graphe de mots avec R, l'utilisation des packages **`R.temis`**, **`dplyr`** et **`tibble`** s'avèrent particulièrement utiles.  \n Le package `R.temis` fournit des fonctions pour les traitements de statistiques textuelles comme la création d'un tableau lexical ou le calcul d'occurences des mots). De son côté, `dplyr` fournit des fonctions pour la manipulation de données, telles que la sélection de colonnes, le filtrage de données et l'agrégation de données, qui sont utilisées dans le code pour nettoyer et préparer les données textuelles. Enfin, `tibble` fournit une classe de données pour stocker des données tabulaires, qui est plus efficace que la classe de données par défaut de R. Cette classe de données est utilisée dans le code pour stocker les données textuelles nettoyées et préparées.\n---\n\n\n\n**Documentation**:  \n\n- [R.temis](http://rtemis.hypotheses.org/)\n- [[dplyr - Officiel](http://dplyr.tidyverse.org/)] [[dplyr - Julien Barnier](http://juba.github.io/tidyverse/10-dplyr.html)] \n- [tibble](https://tibble.tidyverse.org/)\n\n\n```markdown\ninstall.packages(R.temis)\ninstall.packages(dplyr) # ou (tidyverse)\ninstall.packages(tibble) # ou (tidyverse)\n```\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Appel des packages (à installer si besoin)\nlibrary(\"R.temis\")\nlibrary(\"dplyr\") \nlibrary(\"tibble\")\n```\n:::\n\n\nConcernant le choix du corpus, nous avons sélectionné les voeux prononcés par François Hollande de 2013 à 2017. Les textes retranscrits dans des fichiers de type texte (.txt) et placé dans un seul dossier nommé *dossier_de_texte*. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# import des textes avec la fonction import_corpus\n\ncorpus1 <- import_corpus(\"dossier_de_texte\", format=\"txt\", language =\"fr\")\n\n# découpage en unités textuelles de 5 paragraphes pour un meilleur rendu \n\ncorpus<-split_documents(corpus1, 5, preserveMetadata = TRUE)\n```\n:::\n\n\n\nLes *stop words* (ou mots vides) sont des mots très courants d’une langue comme les prépositions, les articles, les pronoms, etc., qui sont souvent omis lors de l’analyse car en général ils ne portent pas de sens important pour la compréhension globale du texte.\nLa fonction **`build_dtm`** est utilisée pour construire une matrice de termes-document (ou **tableau lexical**) à partir d’un corpus de textes. La matrice de termes-document (ou Tableau Lexical) est une représentation quantitative d'un corpus de textes, où chaque colonne représente un terme et chaque ligne représente un document. Ici les documents sont les unités textuelles (composées de 5) paragraphes.\n\nOn choisit de supprimer les mots vides.\n\nOn passe maintenant à la création du tableau lexical (dtm) sans mots outils et avec les mots d’au moins 1 lettre.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#on crée le tableau lexical avec la fonction build_dtm\n\ndtm <-build_dtm(corpus, remove_stopwords = T, min_length = 1)\n\n# Calcul des occurrences des mots dans le corpus de textes\n\nfrequent_terms(dtm) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Global occ.  Global %\nfrance                66 1.9446081\na                     54 1.5910430\nplus                  48 1.4142605\nannée                 33 0.9723041\nêtre                  29 0.8544490\naussi                 28 0.8249853\npays                  24 0.7071302\ntous                  24 0.7071302\ntout                  24 0.7071302\ncontre                19 0.5598114\ncompatriotes          18 0.5303477\nfaire                 18 0.5303477\ndoit                  17 0.5008839\nface                  16 0.4714202\ntoutes                16 0.4714202\nchers                 15 0.4419564\nveux                  15 0.4419564\neurope                14 0.4124926\ncomme                 13 0.3830289\nemploi                13 0.3830289\nmonde                 13 0.3830289\nconfiance             12 0.3535651\nparce                 12 0.3535651\nrépublique            12 0.3535651\nceux                  11 0.3241014\n```\n:::\n:::\n\n\nLe mot *france* est prononcé 66 fois dans l'ensemble des 5 discours et représente 1,95% des occurences totales.\n\nOn va maintenant affiner l'analyse.\n\nOn aimerait aussi retirer les mots *a* et *plus* et rassembler sous un même mot  les termes *tout*, *toutes* et *tous* en *tous.tes* à titre d’exemple.\n\nOn crée le dictionnaire qui affiche les mots initiaux et les racines des mots (Term). Il va servir à lemmatiser le corpus.\n\nLa lemmatisation consiste à remplacer les mots initiaux par des termes: la racine des mots ou une forme personnalisée (comme c'est le cas ici)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Création d'un dictionnaire de mot\ndic <-dictionary(dtm) \n\ndic2 = dic %>%\n  rownames_to_column(var=\"word\") %>% \n  mutate(Term = word)\n\nrow.names(dic2) <- dic2$word\n\n# Remplacer les mots spécifiés par tous.tes\ndic2$Term[dic2$word == \"toutes\"] <- \"tous.tes\"\ndic2$Term[dic2$word == \"tout\"] <- \"tous.tes\"\ndic2$Term[dic2$word == \"tous\"] <- \"tous.tes\"\n\n# Lemmatisation\ndtmlem <-combine_terms(dtm, dic2)\n\n# Ensemble de mots à retirer\nmots_a_retirer <- c(\"a\", \"plus\")\n\n# Suppression de mots dans le tableau lexical\ndtm2<-dtmlem[, !colnames(dtmlem) %in% mots_a_retirer]\n\n\nfrequent_terms(dtm2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Global occ.  Global %\nfrance                66 2.0048603\ntous.tes              64 1.9441069\nannée                 33 1.0024301\nêtre                  29 0.8809235\naussi                 28 0.8505468\npays                  24 0.7290401\ncontre                19 0.5771567\ncompatriotes          18 0.5467801\nfaire                 18 0.5467801\ndoit                  17 0.5164034\nface                  16 0.4860267\nchers                 15 0.4556501\nveux                  15 0.4556501\neurope                14 0.4252734\ncomme                 13 0.3948967\nemploi                13 0.3948967\nmonde                 13 0.3948967\nconfiance             12 0.3645200\nparce                 12 0.3645200\nrépublique            12 0.3645200\nceux                  11 0.3341434\nentre                 11 0.3341434\nentreprises           11 0.3341434\nlà                    11 0.3341434\nsoir                  11 0.3341434\n```\n:::\n:::\n\n\nOn voit que les occurences du \"tous.tes\" (64) correspondent bien à la somme des occurences de tout (24), tous(24) et toutes(16).\n\n\nOn passe à l'affichage du Nuage de mot\n\nCe graphique permet de visualiser les mots les plus **occurents** d’un corpus de textes. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# On affiche au maximum 50 mots et les mots d’au moins une occurrence\ncloud<-word_cloud(dtm2, color= 'black', min.freq=1,n =50) \ntitle(main = \"Mots les plus fréquents dans les discours de F.Hollande entre 2013 et 2017\")\n```\n\n::: {.cell-output-display}\n![](nuage_de_mot_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nLa taille de la police est proportionelle à l'occurence du *mot*: France (66), année (33) et entreprise (11).\n\n\nEt pour finir, l'affichage d’un graphe de mots\n\nLa fonction **`terms_graph`** du package R.temis permet de générer un réseau de mots qui est affiché dans une fenêtre interactive igraph. Les termes ou mots sont représentés par des sommets (ou nœuds) du graphe, les liens représentent les **cooccurrences** entre les mots les plus fréquents. Leur placement dans l’espace graphique est déterminé par un algorithme d’énergie. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Créer un graphique d'analyse de co-occurrences de termes\n\nTree<-terms_graph(dtm2, min_occ = 10, interactive = T,\n            vertex.size = 0.01, vertex.color = \"lightblue\",\n            label.cex = 0.1)\n```\n:::\n\n\n![](tree.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "nuage_de_mot_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}